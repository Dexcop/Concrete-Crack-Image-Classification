{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b7c14534",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os \n",
    "import numpy as np \n",
    "from sklearn.model_selection import train_test_split\n",
    "from skimage.feature import hog\n",
    "from tqdm import tqdm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6e8d63f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = './dataset/'\n",
    "IMG_WIDTH = 224\n",
    "IMG_HEIGHT = 224\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "89742cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For ORB\n",
    "MAX_FEATURES = 100  # Number of max keypoint that will try\n",
    "ORB_DESC_SIZE = 32      # ORB Descriptor Size (selalu 32 byte)\n",
    "\n",
    "# For AKAZE\n",
    "AKAZE_DESC_SIZE = 61"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3344170b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting file paths and labels\n",
      "Total images: 40000\n",
      "Training test size: 30000\n",
      "Testing test size: 10000\n"
     ]
    }
   ],
   "source": [
    "print('Getting file paths and labels')\n",
    "\n",
    "image_paths = []\n",
    "labels = []\n",
    "\n",
    "positive_path = os.path.join(DATASET_PATH, 'Positive')\n",
    "negative_path = os.path.join(DATASET_PATH, 'Negative')\n",
    "\n",
    "for filename in os.listdir(positive_path):\n",
    "    image_paths.append(os.path.join(positive_path, filename))\n",
    "    labels.append(1)\n",
    "    \n",
    "for filename in os.listdir(negative_path):\n",
    "    image_paths.append(os.path.join(negative_path, filename))\n",
    "    labels.append(0)\n",
    "    \n",
    "image_paths = np.array(image_paths)\n",
    "labels = np.array(labels)\n",
    "\n",
    "X_train_paths, X_test_paths, y_train, y_test = train_test_split(\n",
    "    image_paths, labels, test_size=0.25, random_state=42, stratify=labels\n",
    ")\n",
    "\n",
    "print(f'Total images: {len(image_paths)}')\n",
    "print(f'Training test size: {len(X_train_paths)}')\n",
    "print(f'Testing test size: {len(X_test_paths)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2977c325",
   "metadata": {},
   "source": [
    "#### HOG (Histogram of Oriented Gradients) Pipeline => Global Feature Descriptor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "51cdb49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_generator_hog(image_paths, labels, batch_size):\n",
    "    num_samples = len(image_paths)\n",
    "    \n",
    "    while True:\n",
    "        indices = np.arange(num_samples)\n",
    "\n",
    "        # Dishuffle biar gak terjadi data leakage\n",
    "        np.random.shuffle(indices)\n",
    "        \n",
    "        shuffled_paths = image_paths[indices]\n",
    "        shuffled_labels = labels[indices]\n",
    "        \n",
    "        for i in range(0, num_samples, batch_size):\n",
    "            batch_paths = shuffled_paths[i:i+batch_size]\n",
    "            batch_labels = shuffled_labels[i:i+batch_size]\n",
    "            \n",
    "            batch_features = []\n",
    "            \n",
    "            for img_path in batch_paths:\n",
    "                image = cv2.imread(img_path)\n",
    "                image = cv2.resize(image, (IMG_WIDTH, IMG_HEIGHT)) # Ini wajib karena HOG menghasilkan vektor fitur dengan panjang yang bergantung pada ukuran gambar. Semua gambar harus berukuran sama agar fiturnya bisa dibandingkan.\n",
    "                \n",
    "                gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "                \n",
    "                features = hog( gray_image, \n",
    "                                # Fungsi dari pixel per cell karena di dalam setiap sel, HOG menghitung \"arah\" gradien (perubahan dari gelap ke terang) dan membuat histogram kecil. \n",
    "                                # Intinya, ini menangkap struktur lokal seperti tepi atau sudut dalam area 16x16 piksel.\n",
    "\n",
    "                                pixels_per_cell=(16, 16), \n",
    "                                # Blok digunakan untuk normalisasi kontras. Ini membuat fitur HOG lebih tahan terhadap perubahan pencahayaan.\n",
    "                                # Seperti contohnya retakan beton di bawah sinar matahari terik dan di bawah bayangan akan memiliki fitur HOG yang mirip karena adanya normalisasi ini.                  \n",
    "                               \n",
    "                               cells_per_block=(2, 2),\n",
    "                               visualize=False,\n",
    "                               block_norm='L2-Hys',\n",
    "                               # Metode yang digunakan untuk normalisasi di dalam blok. 'L2-Hys' (L2-Hysteresis) adalah metode standar yang sangat populer dan bekerja dengan baik. \n",
    "                               # Ia melakukan normalisasi L2, membatasi nilai maksimum (clipping), lalu menormalisasi lagi.\n",
    "                               \n",
    "                               feature_vector=True\n",
    "                               # Ini memastikan output dari fungsi hog adalah satu vektor 1D yang datar (flattened). \n",
    "                               # Jika False, outputnya akan berupa array multi-dimensi. \n",
    "                               # Untuk dimasukkan ke model klasifikasi standar (seperti SVM atau Random Forest), kita memerlukan vektor 1D.\n",
    "                               ) \n",
    "                \n",
    "                batch_features.append(features)\n",
    "            \n",
    "            yield np.array(batch_features), np.array(batch_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff66df0",
   "metadata": {},
   "source": [
    "#### ORB (Oriented FAST and Rotated BRIEF) Pipeline => Local Feature Descriptor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "84ff3c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_generator_orb(image_paths, labels, batch_size):\n",
    "    num_samples = len(image_paths)\n",
    "    \n",
    "    orb = cv2.ORB_create(nfeatures=MAX_FEATURES)\n",
    "    # Ini memberitahu ORB untuk mencari maksimal MAX_FEATURES (misalnya 100) titik paling \"menarik\" di gambar. \n",
    "    # Ini adalah langkah pertama untuk memastikan output kita konsisten.\n",
    "    \n",
    "    while True:\n",
    "        indices = np.arange(num_samples)\n",
    "        np.random.shuffle(indices)\n",
    "        \n",
    "        shuffled_paths = image_paths[indices]\n",
    "        shuffled_labels = labels[indices]\n",
    "        \n",
    "        for i in range(0, num_samples, batch_size):\n",
    "            batch_paths = shuffled_paths[i:i+batch_size]\n",
    "            batch_labels = shuffled_labels[i:i+batch_size]\n",
    "            \n",
    "            batch_features = []\n",
    "            \n",
    "            for img_path in batch_paths:\n",
    "                image = cv2.imread(img_path)\n",
    "                image = cv2.resize(image, (IMG_WIDTH, IMG_HEIGHT))\n",
    "                \n",
    "                gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "                \n",
    "                kps, des = orb.detectAndCompute(gray_image, None)\n",
    "                # kps => keypoints\n",
    "                # des => descriptor\n",
    "                \n",
    "                flat_features = np.zeros(MAX_FEATURES * ORB_DESC_SIZE, dtype=np.float32)\n",
    "                # awal kita buat semuanya itu menjadi [0,0]\n",
    "                if des is not None:\n",
    "                    num_descriptors = min(len(des), MAX_FEATURES) # capping => tidak ambil lebih dari batas feature jumlah maksimal\n",
    "                    flat_features[:num_descriptors * ORB_DESC_SIZE] = des[:num_descriptors].ravel()\n",
    "                    # des[:num_descriptors] mengambil semua deskriptor yang valid (maksimal 100).\n",
    "\n",
    "                    # .ravel() \"meratakan\" array (100, 32) menjadi satu baris panjang (3200,).\n",
    "\n",
    "                    # flat_features[...] = ... menyalin nilai-nilai ini ke dalam \"wadah\" nol yang kita siapkan.\n",
    "                flat_features /= (np.linalg.norm(flat_features) + 1e-6)\n",
    "                # flat_features /= ...: Vektor yang sudah jadi kemudian dinormalisasi. \n",
    "                # Ini membuat fitur lebih tahan terhadap perbedaan pencahayaan antar gambar.\n",
    "                \n",
    "                batch_features.append(flat_features)\n",
    "            \n",
    "            yield np.array(batch_features), np.array(batch_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89fe614",
   "metadata": {},
   "source": [
    "#### MSER (Maximally Stable Extermal Regions) => Global Feature Detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f369d8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_generator_mser(image_paths, labels, batch_size):\n",
    "    num_samples = len(image_paths)\n",
    "\n",
    "    mser = cv2.MSER_create()\n",
    "    \n",
    "    while True:\n",
    "        indices = np.arange(num_samples)\n",
    "        np.random.shuffle(indices)\n",
    "        \n",
    "        shuffled_paths = image_paths[indices]\n",
    "        shuffled_labels = labels[indices]\n",
    "        \n",
    "        for i in range(0, num_samples, batch_size):\n",
    "            batch_paths = shuffled_paths[i:i+batch_size]\n",
    "            batch_labels = shuffled_labels[i:i+batch_size]\n",
    "            \n",
    "            batch_features = []\n",
    "            \n",
    "            for img_path in batch_paths:\n",
    "                image = cv2.imread(img_path)\n",
    "                image = cv2.resize(image, (IMG_WIDTH, IMG_HEIGHT))\n",
    "                \n",
    "                gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "                \n",
    "                # Deteksi region MSER\n",
    "                # MSER membutuhkan gambar 8-bit single-channel\n",
    "                regions, _ = mser.detectRegions(gray_image)\n",
    "                \n",
    "                num_regions = len(regions)\n",
    "                \n",
    "                if num_regions > 0:\n",
    "                    # Hitung properti statistik dari region\n",
    "                    areas = [cv2.contourArea(r) for r in regions]\n",
    "                    avg_area = np.mean(areas)\n",
    "                    std_area = np.std(areas)\n",
    "                    # Rasio total area region terhadap area gambar\n",
    "                    total_area_ratio = np.sum(areas) / (IMG_WIDTH * IMG_HEIGHT)\n",
    "                    \n",
    "                    # Gabungkan menjadi satu feature vector\n",
    "                    features = np.array([num_regions, avg_area, std_area, total_area_ratio])\n",
    "                else:\n",
    "                    # Jika tidak ada region yang terdeteksi, fiturnya nol semua\n",
    "                    features = np.zeros(4, dtype=np.float32)\n",
    "\n",
    "                batch_features.append(features)\n",
    "            \n",
    "            yield np.array(batch_features), np.array(batch_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a24ca9c",
   "metadata": {},
   "source": [
    "#### AKAZE (Accelerated-KAZE) Pipeline => Local Feature Detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "7337dd5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_generator_akaze(image_paths, labels, batch_size):\n",
    "    num_samples = len(image_paths)\n",
    "\n",
    "    akaze = cv2.AKAZE_create()\n",
    "    \n",
    "    while True:\n",
    "        indices = np.arange(num_samples)\n",
    "        np.random.shuffle(indices)\n",
    "        \n",
    "        shuffled_paths = image_paths[indices]\n",
    "        shuffled_labels = labels[indices]\n",
    "        \n",
    "        for i in range(0, num_samples, batch_size):\n",
    "            batch_paths = shuffled_paths[i:i+batch_size]\n",
    "            batch_labels = shuffled_labels[i:i+batch_size]\n",
    "            \n",
    "            batch_features = []\n",
    "            \n",
    "            for img_path in batch_paths:\n",
    "                image = cv2.imread(img_path)\n",
    "                image = cv2.resize(image, (IMG_WIDTH, IMG_HEIGHT))\n",
    "                \n",
    "                gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "                kps, des = akaze.detectAndCompute(gray_image, None)\n",
    "\n",
    "                flat_features = np.zeros(MAX_FEATURES * AKAZE_DESC_SIZE, dtype=np.float32)\n",
    "                \n",
    "                if des is not None:\n",
    "                    num_descriptors = min(len(des), MAX_FEATURES)\n",
    "\n",
    "                    flat_features[:num_descriptors * AKAZE_DESC_SIZE] = des[:num_descriptors].ravel()\n",
    "                \n",
    "                flat_features /= (np.linalg.norm(flat_features) + 1e-6)\n",
    "                \n",
    "                batch_features.append(flat_features)\n",
    "            \n",
    "            yield np.array(batch_features), np.array(batch_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d7a8e27a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verification of HOG Pipeline\n",
      "fetching one batch of feature vectors to test\n",
      "pipeline complete, ready for training\n",
      "shape of one batch of features: (32, 6084)\n",
      "shape of one batch of labels: (32,)\n",
      "example feature vector (first image in batch:\n",
      " [0.22943039 0.07381583 0.22943039 ... 0.23797284 0.23797284 0.18398376])\n"
     ]
    }
   ],
   "source": [
    "# Verification\n",
    "print('Verification of HOG Pipeline')\n",
    "train_gen_hog = feature_generator_hog(X_train_paths, y_train, BATCH_SIZE)\n",
    "\n",
    "print('fetching one batch of feature vectors to test')\n",
    "sample_batch_features_hog, sample_batch_labels_hog = next(train_gen_hog) \n",
    "\n",
    "print('pipeline complete, ready for training')\n",
    "print(f'shape of one batch of features: {sample_batch_features_hog.shape}') # 32 per batch and 10 length\n",
    "print(f'shape of one batch of labels: {sample_batch_labels_hog.shape}')  # 32 per batch\n",
    "print(f'example feature vector (first image in batch:\\n {sample_batch_features_hog[0]})') # 10 arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "44dc3bcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verification of ORB Pipeline\n",
      "fetching one batch of feature vectors to test\n",
      "pipeline complete, ready for training\n",
      "shape of one batch of features: (32, 3200)\n",
      "shape of one batch of labels: (32,)\n",
      "example feature vector (first image in batch:\n",
      " [0.01405322 0.01565422 0.04073654 ... 0.         0.         0.        ])\n"
     ]
    }
   ],
   "source": [
    "# Verification\n",
    "print('Verification of ORB Pipeline')\n",
    "train_gen_orb = feature_generator_orb(X_train_paths, y_train, BATCH_SIZE)\n",
    "\n",
    "print('fetching one batch of feature vectors to test')\n",
    "sample_batch_features_orb, sample_batch_labels_orb = next(train_gen_orb) \n",
    "\n",
    "print('pipeline complete, ready for training')\n",
    "print(f'shape of one batch of features: {sample_batch_features_orb.shape}') # 32 per batch and 10 length\n",
    "print(f'shape of one batch of labels: {sample_batch_labels_orb.shape}')  # 32 per batch\n",
    "print(f'example feature vector (first image in batch:\\n {sample_batch_features_orb[0]})') # 10 arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "4f6f46a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verification of MSER Pipeline\n",
      "fetching one batch of feature vectors to test\n",
      "pipeline complete, ready for training\n",
      "shape of one batch of features: (32, 4)\n",
      "shape of one batch of labels: (32,)\n",
      "example feature vector (first image in batch:\n",
      " [3.60000000e+01 9.73923611e+03 1.00767035e+04 6.98765346e+00])\n"
     ]
    }
   ],
   "source": [
    "# Verification\n",
    "print('Verification of MSER Pipeline')\n",
    "train_gen_mser = feature_generator_mser(X_train_paths, y_train, BATCH_SIZE)\n",
    "\n",
    "print('fetching one batch of feature vectors to test')\n",
    "sample_batch_features_mser, sample_batch_labels_mser = next(train_gen_mser) \n",
    "\n",
    "print('pipeline complete, ready for training')\n",
    "print(f'shape of one batch of features: {sample_batch_features_mser.shape}') # 32 per batch and 10 length\n",
    "print(f'shape of one batch of labels: {sample_batch_labels_mser.shape}')  # 32 per batch\n",
    "print(f'example feature vector (first image in batch:\\n {sample_batch_features_mser[0]})') # 10 arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "91a649cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verification of AKAZE Pipeline\n",
      "fetching one batch of feature vectors to test\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pipeline complete, ready for training\n",
      "shape of one batch of features: (32, 6100)\n",
      "shape of one batch of labels: (32,)\n",
      "example feature vector (first image in batch:\n",
      " [0. 0. 0. ... 0. 0. 0.])\n"
     ]
    }
   ],
   "source": [
    "# Verification\n",
    "print('Verification of AKAZE Pipeline')\n",
    "train_gen_akaze = feature_generator_akaze(X_train_paths, y_train, BATCH_SIZE)\n",
    "\n",
    "print('fetching one batch of feature vectors to test')\n",
    "sample_batch_features_akaze, sample_batch_labels_akaze = next(train_gen_akaze) \n",
    "\n",
    "print('pipeline complete, ready for training')\n",
    "print(f'shape of one batch of features: {sample_batch_features_akaze.shape}') # 32 per batch and 10 length\n",
    "print(f'shape of one batch of labels: {sample_batch_labels_akaze.shape}')  # 32 per batch\n",
    "print(f'example feature vector (first image in batch:\\n {sample_batch_features_akaze[0]})') # 10 arrays"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Computer Vision)",
   "language": "python",
   "name": "comvis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
